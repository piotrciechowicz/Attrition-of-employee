# -*- coding: utf-8 -*-
"""Attrition of employee.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12_uiWwK_nfZ8UMTZg_aCdkzFresd0jHv
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""#Odczyt danych"""

df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/df1.csv')
df1.head()

df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/df2.csv')
df2.head()

df2.iloc[:,1:]

df = pd.concat([df1,df2.iloc[:,1:]], axis = 1)
df.sample(5)

df.describe()

df.isnull().sum()

df_2 = df.dropna(axis=0)

df_2.dtypes

"""##Usunięcie kolumn nieistotnych """

df_3 = df_2.drop(['StandardHours','EmployeeNumber','Over18','EmployeeCount','MonthlyRate','DailyRate','YearlyIncome','HourlyRate'], inplace=False, axis=1)
df_3.head()

df_3['TrainingTimesLastYear']

df_3.dtypes

"""## Uśrednienie wartości odstających"""

mean_age = df_3['Age'][df_3['Age']<60].mean()
df_3['Age'] = np.where(df_3['Age']<60, df_3['Age'], mean_age)
sns.distplot(df_3.Age, bins =10)

mean_DistanceFromHome = df_3['DistanceFromHome'][df_3['DistanceFromHome']<30].mean()
df_3['DistanceFromHome'] = np.where(df_3['DistanceFromHome']<30, df_3['DistanceFromHome'], mean_DistanceFromHome)
sns.distplot(df_3.DistanceFromHome, bins =10)

mean_TotalWorkingYears = df_3['TotalWorkingYears'][df_3['TotalWorkingYears']<50].mean()
df_3['TotalWorkingYears'] = np.where(df_3['TotalWorkingYears']<50, df_3['TotalWorkingYears'], mean_TotalWorkingYears)
sns.distplot(df_3.TotalWorkingYears, bins =10)

mean_YearsAtCompany = df_3['YearsAtCompany'][df_3['YearsAtCompany']<20].mean()
df_3['YearsAtCompany'] = np.where(df_3['YearsAtCompany']<20, df_3['YearsAtCompany'], mean_YearsAtCompany)
sns.distplot(df_3.YearsAtCompany, bins =10)

mean_YearsInCurrentRole = df_3['YearsInCurrentRole'][df_3['YearsInCurrentRole']<20].mean()
df_3['YearsInCurrentRole'] = np.where(df_3['YearsInCurrentRole']<20, df_3['YearsInCurrentRole'], mean_YearsInCurrentRole)
sns.distplot(df_3.YearsInCurrentRole, bins =10)

mean_YearsSinceLastPromotion = df_3['YearsSinceLastPromotion'][df_3['YearsSinceLastPromotion']<10].mean()
df_3['YearsSinceLastPromotion'] = np.where(df_3['YearsSinceLastPromotion']<10, df_3['YearsSinceLastPromotion'], mean_YearsSinceLastPromotion)
sns.distplot(df_3.YearsSinceLastPromotion, bins =10)

sns.distplot(df_3.MonthlyIncome, bins =10)

mean_YearsWithCurrManager = df_3['YearsWithCurrManager'][df_3['YearsWithCurrManager']<10].mean()
df_3['YearsWithCurrManager'] = np.where(df_3['YearsWithCurrManager']<10, df_3['YearsWithCurrManager'], mean_YearsWithCurrManager)
sns.distplot(df_3.YearsWithCurrManager, bins =10)

num_cols = df_3.columns[df_3.dtypes.apply(lambda c: np.issubdtype(c, np.number))]
num_cols

df_4 = pd.get_dummies(df_3)
df_4

"""## Heatmap korelacji zmiennych"""

corr = df_4.corr()
sns.heatmap(np.abs(corr))

df_4.dtypes

df_5 = df_4.drop(['Department_Research & Development','Department_Sales','Department_Human Resources','WorkLifeBalance'], inplace=False, axis=1)

df_5

corr = df_5.corr()
sns.heatmap(np.abs(corr))

y = df_5['Attrition_Yes']
X = df_5.drop(['Attrition_Yes','Attrition_No'], inplace=False, axis=1)

"""#Modelowanie

##Zbiór testowy i treningowy
"""

from sklearn.model_selection import train_test_split

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

X_sc_1= X_train.loc[:,['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction','JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome','NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating','RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears','TrainingTimesLastYear', 'YearsAtCompany','YearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager']]

"""##Standaryzacja"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_sc_1)
X_sc = scaler.transform(X_train.loc[:,['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction','JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome','NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating','RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears','TrainingTimesLastYear', 'YearsAtCompany','YearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager']])
X_sc_train_1 = pd.DataFrame(data = X_sc,
                           columns = X_sc_1.columns,
                           index = X_sc_1.index)
X_2 = X_train.drop(['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction','JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome','NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating','RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears','TrainingTimesLastYear', 'YearsAtCompany','YearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager'], inplace=False, axis=1)
X_train_st = pd.concat([X_2, X_sc_train_1], axis = 1)

X_test_1 = X_test.loc[:,['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction','JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome','NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating','RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears','TrainingTimesLastYear', 'YearsAtCompany','YearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager']]
X_test_1_sc = scaler.transform(X_test_1)
X_test_1_sc = pd.DataFrame(data = X_test_1_sc,
                           columns = X_test_1.columns,
                           index = X_test_1.index)
X_test_2 = X_test.drop(['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction','JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome','NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating','RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears','TrainingTimesLastYear', 'YearsAtCompany','YearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager'], inplace=False, axis=1)
X_test_st = pd.concat([X_test_2, X_test_1_sc], axis = 1)
X_test_st

"""##Redukcja wymiarowości"""

from sklearn.decomposition import PCA
model = PCA()

pca_features_train = model.fit_transform(X_train_st)
pca_features_test = model.transform(X_test_st)

features = range(model.n_components_)
plt.bar(features, model.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')

#X_train_st = pca_features_train[:,0:5]
#X_test_st = pca_features_test[:,0:5]

"""##Regresja logistyczna"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

param = {
        'penalty':('l1', 'l2', 'elasticnet', 'none'),
         'C':[0.2 , 0.4 , 0.6 , 0.8 , 1 ]
        }

LR = LogisticRegression(random_state=30, solver = 'saga')
grid_search_lr = GridSearchCV(LR, param, scoring = 'accuracy').fit(X_train_st, y_train)

y_pred_train = grid_search_lr.predict(X_train_st)
y_pred = grid_search_lr.predict(X_test_st)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy_train = accuracy_score(y_train, y_pred_train)
precision_train = precision_score(y_train, y_pred_train)
recall_train = recall_score(y_train, y_pred_train)
f1_train = f1_score(y_train, y_pred_train)

print(f'Accuracy: {accuracy_train}')
print(f'Precision: {precision_train}')
print(f'Recall: {recall_train}')
print(f'F1-Score: {f1_train}')

accuracy_test = accuracy_score(y_test, y_pred)
precision_test = precision_score(y_test, y_pred)
recall_test = recall_score(y_test, y_pred)
f1_test = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy_test}')
print(f'Precision: {precision_test}')
print(f'Recall: {recall_test}')
print(f'F1-Score: {f1_test}')

"""##GaussianNB"""

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(X_train_st, y_train)

y_pred_train = gnb.predict(X_train_st)
y_pred = gnb.predict(X_test_st)

accuracy_train = accuracy_score(y_train, y_pred_train)
precision_train = precision_score(y_train, y_pred_train)
recall_train = recall_score(y_train, y_pred_train)
f1_train = f1_score(y_train, y_pred_train)

print(f'Accuracy: {accuracy_train}')
print(f'Precision: {precision_train}')
print(f'Recall: {recall_train}')
print(f'F1-Score: {f1_train}')

accuracy_test = accuracy_score(y_test, y_pred)
precision_test = precision_score(y_test, y_pred)
recall_test = recall_score(y_test, y_pred)
f1_test = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy_test}')
print(f'Precision: {precision_test}')
print(f'Recall: {recall_test}')
print(f'F1-Score: {f1_test}')

"""##BernoulliNB"""

from sklearn.naive_bayes import BernoulliNB

bnb = BernoulliNB()
bnb.fit(X_train_st, y_train)

y_pred_train = bnb.predict(X_train_st)
y_pred = bnb.predict(X_test_st)

accuracy_train = accuracy_score(y_train, y_pred_train)
precision_train = precision_score(y_train, y_pred_train)
recall_train = recall_score(y_train, y_pred_train)
f1_train = f1_score(y_train, y_pred_train)

print(f'Accuracy: {accuracy_train}')
print(f'Precision: {precision_train}')
print(f'Recall: {recall_train}')
print(f'F1-Score: {f1_train}')

accuracy_test = accuracy_score(y_test, y_pred)
precision_test = precision_score(y_test, y_pred)
recall_test = recall_score(y_test, y_pred)
f1_test = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy_test}')
print(f'Precision: {precision_test}')
print(f'Recall: {recall_test}')
print(f'F1-Score: {f1_test}')

"""##Lasy Losowe"""

#bez standaryzacji

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()

param= {'max_depth':[2, 6, 3, 1, 10],
        'min_samples_leaf':[1, 4, 3, 2, 5],
        'min_impurity_decrease':[0.2, 0.5, 0.4]}



grid_search_clf = GridSearchCV(clf, param, scoring = 'accuracy').fit(X_train_st, y_train)

y_pred_train = grid_search_clf.predict(X_train_st)
y_pred = grid_search_clf.predict(X_test_st)

accuracy_train = accuracy_score(y_train, y_pred_train)
precision_train = precision_score(y_train, y_pred_train)
recall_train = recall_score(y_train, y_pred_train)
f1_train = f1_score(y_train, y_pred_train)

print(f'Accuracy: {accuracy_train}')
print(f'Precision: {precision_train}')
print(f'Recall: {recall_train}')
print(f'F1-Score: {f1_train}')

accuracy_test = accuracy_score(y_test, y_pred)
precision_test = precision_score(y_test, y_pred)
recall_test = recall_score(y_test, y_pred)
f1_test = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy_test}')
print(f'Precision: {precision_test}')
print(f'Recall: {recall_test}')
print(f'F1-Score: {f1_test}')

print("Best CV params", grid_search_clf.best_params_)
grid_search_clf.best_estimator_.feature_importances_